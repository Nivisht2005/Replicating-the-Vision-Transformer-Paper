# Replicating-the-Vision-Transformer-Paper
This repository contains a PyTorch-based implementation of the Vision Transformer (ViT) model as proposed in the paper "An Image is Worth 16x16 Words". The project covers image tokenization via patching, positional encoding, transformer encoder blocks, and training on image classification tasks with performance evaluation.
